{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sid0nair/Crack-propagation-predictor/blob/main/APL405.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjrOSMqJ2_pa",
        "outputId": "588694c6-9dd2-402a-bd54-16b9b8986930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "UkqM5i9s4oE0",
        "outputId": "f00744cf-10d3-4101-94e6-708f2671a1c0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/input'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-92b550473c75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0minput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/data/input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/data/output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrackDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-92b550473c75>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dir, output_dir, target_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/input'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, input_dir, output_dir, target_size=(256, 256)):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.input_files = sorted(os.listdir(input_dir))\n",
        "        self.output_files = sorted(os.listdir(output_dir))\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_path = os.path.join(self.input_dir, self.input_files[idx])\n",
        "        output_path = os.path.join(self.output_dir, self.output_files[idx])\n",
        "\n",
        "        # Read images in color\n",
        "        input_img = cv2.imread(input_path, cv2.IMREAD_COLOR)\n",
        "        output_img = cv2.imread(output_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Convert from BGR (default in OpenCV) to RGB\n",
        "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
        "        output_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize images to the target dimensions\n",
        "        input_img = cv2.resize(input_img, self.target_size)\n",
        "        output_img = cv2.resize(output_img, self.target_size)\n",
        "\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        input_img = input_img.astype('float32') / 255.0\n",
        "        output_img = output_img.astype('float32') / 255.0\n",
        "\n",
        "        # Convert from HWC to CHW format\n",
        "        input_img = np.transpose(input_img, (2, 0, 1))\n",
        "        output_img = np.transpose(output_img, (2, 0, 1))\n",
        "\n",
        "        return {\n",
        "            'input': torch.tensor(input_img, dtype=torch.float32),\n",
        "            'output': torch.tensor(output_img, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Define directories and create the dataset\n",
        "input_dir = '/content/data/input'\n",
        "output_dir = '/content/data/output'\n",
        "dataset = CrackDataset(input_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQUigm4a7gZB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGWfNufs5f7_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(UNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.conv1(x)\n",
        "        p1 = self.pool1(c1)\n",
        "        c2 = self.conv2(p1)\n",
        "        p2 = self.pool2(c2)\n",
        "        c3 = self.conv3(p2)\n",
        "\n",
        "        # Decoder\n",
        "        u2 = self.up2(c3)\n",
        "        merge2 = torch.cat([u2, c2], dim=1)\n",
        "        c4 = self.conv4(merge2)\n",
        "        u1 = self.up1(c4)\n",
        "        merge1 = torch.cat([u1, c1], dim=1)\n",
        "        c5 = self.conv5(merge1)\n",
        "\n",
        "        output = self.final(c5)\n",
        "        output = torch.sigmoid(output)  # Ensure outputs are in [0,1]\n",
        "        return output\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels=3, out_channels=3).to(device)\n",
        "\n",
        "# Use MSE loss since we are predicting full-color images\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-v9V1f57q0J"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch['input'].to(device)   # shape: [batch_size, 3, H, W]\n",
        "        targets = batch['output'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Optionally, save model checkpoints at intervals:\n",
        "    # if (epoch + 1) % 10 == 0:\n",
        "    #     torch.save(model.state_dict(), f\"unet_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpomeDI355pO"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch['input'].to(device)\n",
        "        targets = batch['output'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn_ZDAvC6CeT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Retrieve one sample from the test DataLoader\n",
        "    sample_batch = next(iter(test_loader))\n",
        "    input_img = sample_batch['input'][0].cpu().numpy()   # Shape: [3, H, W]\n",
        "    target_img = sample_batch['output'][0].cpu().numpy()\n",
        "    pred_img = model(sample_batch['input'].to(device))[0].cpu().numpy()\n",
        "\n",
        "    # Convert images from CHW to HWC for visualization\n",
        "    input_img = np.transpose(input_img, (1, 2, 0))\n",
        "    target_img = np.transpose(target_img, (1, 2, 0))\n",
        "    pred_img = np.transpose(pred_img, (1, 2, 0))\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(input_img)\n",
        "    plt.title(\"Initial Configuration\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(target_img)\n",
        "    plt.title(\"Ground Truth Final\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(pred_img)\n",
        "    plt.title(\"Predicted Final\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXERFCu2BzP1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention = self.conv(x)\n",
        "        attention = self.sigmoid(attention)\n",
        "        return x * attention.expand_as(x)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class EnhancedUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(EnhancedUNet, self).__init__()\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.attention_blocks = nn.ModuleList()\n",
        "\n",
        "        # Encoder\n",
        "        for feature in features:\n",
        "            self.encoder.append(ConvBlock(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = ConvBlock(features[-1], features[-1]*2)\n",
        "        bottleneck_size = features[-1]*2  # Store this for metadata integration\n",
        "\n",
        "        # Metadata integration - adjusted to match bottleneck size\n",
        "        self.metadata_fc = nn.Sequential(\n",
        "            nn.Linear(2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, bottleneck_size)  # Match bottleneck feature dimension\n",
        "        )\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        for feature in reversed(features):\n",
        "            self.decoder.append(\n",
        "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.decoder.append(ConvBlock(feature*2, feature))\n",
        "            self.attention_blocks.append(AttentionBlock(feature))\n",
        "\n",
        "        # Final output\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, metadata):\n",
        "        # x shape: [batch_size, channels, height, width]\n",
        "        # metadata shape: [batch_size, 2] (notch_length, notch_position)\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        # Print shape info during first forward pass for debugging\n",
        "        debug = False\n",
        "\n",
        "        # Encoder path\n",
        "        for i, enc in enumerate(self.encoder):\n",
        "            x = enc(x)\n",
        "            if debug:\n",
        "                print(f\"Encoder {i} output shape: {x.shape}\")\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        if debug:\n",
        "            print(f\"Bottleneck output shape: {x.shape}\")\n",
        "\n",
        "        # Integrate metadata\n",
        "        batch_size, channels, h, w = x.shape\n",
        "        metadata_features = self.metadata_fc(metadata)  # [batch_size, bottleneck_size]\n",
        "        if debug:\n",
        "            print(f\"Metadata features shape: {metadata_features.shape}\")\n",
        "\n",
        "        # Reshape metadata features to match bottleneck spatial dimensions\n",
        "        metadata_features = metadata_features.view(batch_size, channels, 1, 1).expand(-1, -1, h, w)\n",
        "        if debug:\n",
        "            print(f\"Reshaped metadata features shape: {metadata_features.shape}\")\n",
        "\n",
        "        # Add metadata features to bottleneck features\n",
        "        x = x + 0.1 * metadata_features  # Reduced influence with 0.1 scaling factor\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        skip_connections = skip_connections[::-1]  # Reverse for easier access\n",
        "\n",
        "        for idx in range(0, len(self.decoder), 2):\n",
        "            x = self.decoder[idx](x)  # Upsample\n",
        "            if debug:\n",
        "                print(f\"Decoder upsampled {idx} shape: {x.shape}\")\n",
        "\n",
        "            skip = skip_connections[idx//2]\n",
        "            if debug:\n",
        "                print(f\"Skip connection {idx//2} shape: {skip.shape}\")\n",
        "\n",
        "            # Apply attention to skip connection\n",
        "            attended_skip = self.attention_blocks[idx//2](skip)\n",
        "\n",
        "            # Handle different sizes\n",
        "            if x.shape != attended_skip.shape:\n",
        "                x = F.interpolate(x, size=attended_skip.shape[2:])\n",
        "                if debug:\n",
        "                    print(f\"After interpolation shape: {x.shape}\")\n",
        "\n",
        "            concat_skip = torch.cat((attended_skip, x), dim=1)\n",
        "            if debug:\n",
        "                print(f\"After concat shape: {concat_skip.shape}\")\n",
        "\n",
        "            x = self.decoder[idx+1](concat_skip)\n",
        "            if debug:\n",
        "                print(f\"Decoder block {idx+1} output shape: {x.shape}\")\n",
        "\n",
        "        # Final output\n",
        "        return torch.sigmoid(self.final_conv(x))\n",
        "\n",
        "# Instantiate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EnhancedUNet(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "# Print model summary to check architecture\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3bXrf-E9FVSW",
        "outputId": "735ac886-113b-4e19-a753-ee8d4071a407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 49 samples\n",
            "Successfully loaded a sample from dataset\n",
            "Input shape: torch.Size([3, 256, 256])\n",
            "Output shape: torch.Size([3, 256, 256])\n",
            "Crack mask shape: torch.Size([1, 256, 256])\n",
            "Metadata: notch_length=0.14000000059604645, position=0.0\n",
            "Successfully loaded a batch from dataloader\n",
            "Batch input shape: torch.Size([4, 3, 256, 256])\n",
            "Batch metadata shape: torch.Size([4, 2])\n",
            "Input shape: torch.Size([4, 3, 256, 256])\n",
            "Crack mask shape: torch.Size([4, 1, 256, 256])\n",
            "Metadata shape: torch.Size([4, 2])\n",
            "Output shape: torch.Size([4, 1, 256, 256])\n",
            "Epoch 1, Batch 0/10, Loss: 0.6885\n",
            "Epoch [1/50], Train Loss: 0.3373, Val Loss: 0.6765\n",
            "Model saved at epoch 1 with val_loss: 0.6765\n",
            "Epoch 2, Batch 0/10, Loss: 0.2349\n",
            "Epoch [2/50], Train Loss: 0.2099, Val Loss: 0.7198\n",
            "Epoch 3, Batch 0/10, Loss: 0.1833\n",
            "Epoch [3/50], Train Loss: 0.1650, Val Loss: 0.7776\n",
            "Epoch 4, Batch 0/10, Loss: 0.1438\n",
            "Epoch [4/50], Train Loss: 0.1305, Val Loss: 0.7895\n",
            "Epoch 5, Batch 0/10, Loss: 0.1143\n",
            "Epoch [5/50], Train Loss: 0.1036, Val Loss: 0.8110\n",
            "Epoch 6, Batch 0/10, Loss: 0.0905\n",
            "Epoch [6/50], Train Loss: 0.0829, Val Loss: 0.8031\n",
            "Epoch 7, Batch 0/10, Loss: 0.0741\n",
            "Epoch [7/50], Train Loss: 0.0672, Val Loss: 0.7932\n",
            "Epoch 8, Batch 0/10, Loss: 0.0590\n",
            "Epoch [8/50], Train Loss: 0.0576, Val Loss: 0.7201\n",
            "Epoch 9, Batch 0/10, Loss: 0.0541\n",
            "Epoch [9/50], Train Loss: 0.0526, Val Loss: 0.3310\n",
            "Model saved at epoch 9 with val_loss: 0.3310\n",
            "Epoch 10, Batch 0/10, Loss: 0.0507\n",
            "Epoch [10/50], Train Loss: 0.0483, Val Loss: 0.0617\n",
            "Model saved at epoch 10 with val_loss: 0.0617\n",
            "Epoch 11, Batch 0/10, Loss: 0.0459\n",
            "Epoch [11/50], Train Loss: 0.0445, Val Loss: 0.0676\n",
            "Epoch 12, Batch 0/10, Loss: 0.0416\n",
            "Epoch [12/50], Train Loss: 0.0411, Val Loss: 0.0911\n",
            "Epoch 13, Batch 0/10, Loss: 0.0384\n",
            "Epoch [13/50], Train Loss: 0.0381, Val Loss: 0.0406\n",
            "Model saved at epoch 13 with val_loss: 0.0406\n",
            "Epoch 14, Batch 0/10, Loss: 0.0368\n",
            "Epoch [14/50], Train Loss: 0.0354, Val Loss: 0.0297\n",
            "Model saved at epoch 14 with val_loss: 0.0297\n",
            "Epoch 15, Batch 0/10, Loss: 0.0334\n",
            "Epoch [15/50], Train Loss: 0.0330, Val Loss: 0.0336\n",
            "Epoch 16, Batch 0/10, Loss: 0.0298\n",
            "Epoch [16/50], Train Loss: 0.0308, Val Loss: 0.0183\n",
            "Model saved at epoch 16 with val_loss: 0.0183\n",
            "Epoch 17, Batch 0/10, Loss: 0.0298\n",
            "Epoch [17/50], Train Loss: 0.0288, Val Loss: 0.0195\n",
            "Epoch 18, Batch 0/10, Loss: 0.0285\n",
            "Epoch [18/50], Train Loss: 0.0270, Val Loss: 0.0197\n",
            "Epoch 19, Batch 0/10, Loss: 0.0258\n",
            "Epoch [19/50], Train Loss: 0.0255, Val Loss: 0.0210\n",
            "Epoch 20, Batch 0/10, Loss: 0.0238\n",
            "Epoch [20/50], Train Loss: 0.0241, Val Loss: 0.0184\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Mixed loss function for better crack path prediction\n",
        "class CrackLoss(nn.Module):\n",
        "    def __init__(self, dice_weight=0.5, bce_weight=0.5):\n",
        "        super(CrackLoss, self).__init__()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce_weight = bce_weight\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    def dice_coef(self, y_pred, y_true, smooth=1.0):\n",
        "        y_pred_flat = y_pred.view(-1)\n",
        "        y_true_flat = y_true.view(-1)\n",
        "        intersection = (y_pred_flat * y_true_flat).sum()\n",
        "        return (2. * intersection + smooth) / (y_pred_flat.sum() + y_true_flat.sum() + smooth)\n",
        "\n",
        "    def dice_loss(self, y_pred, y_true):\n",
        "        return 1 - self.dice_coef(y_pred, y_true)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        dice = self.dice_loss(y_pred, y_true)\n",
        "        bce = self.bce_loss(y_pred, y_true)\n",
        "        return self.bce_weight * bce + self.dice_weight * dice\n",
        "\n",
        "# Setup dataset and dataloader\n",
        "# Ensure these directories exist\n",
        "input_dir = '/content/data/input'\n",
        "output_dir = '/content/data/output'\n",
        "dataset = CrackDataset(input_dir, output_dir, target_size=(256, 256))\n",
        "\n",
        "# Print dataset size\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "\n",
        "# Try fetching a sample to make sure dataset works\n",
        "try:\n",
        "    sample = dataset[0]\n",
        "    print(\"Successfully loaded a sample from dataset\")\n",
        "    print(f\"Input shape: {sample['input'].shape}\")\n",
        "    print(f\"Output shape: {sample['output'].shape}\")\n",
        "    print(f\"Crack mask shape: {sample['crack_mask'].shape}\")\n",
        "    print(f\"Metadata: notch_length={sample['notch_length']}, position={sample['notch_position']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sample: {e}\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 4  # Reduced from 8 to avoid potential memory issues\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verify dataloader works\n",
        "try:\n",
        "    batch = next(iter(train_loader))\n",
        "    print(\"Successfully loaded a batch from dataloader\")\n",
        "    print(f\"Batch input shape: {batch['input'].shape}\")\n",
        "    print(f\"Batch metadata shape: {torch.stack([batch['notch_length'], batch['notch_position']], dim=1).shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading batch: {e}\")\n",
        "\n",
        "# Setup model, loss function, optimizer, and scheduler\n",
        "model = EnhancedUNet(in_channels=3, out_channels=1).to(device)\n",
        "criterion = CrackLoss(dice_weight=0.7, bce_weight=0.3)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "# Training loop with additional error handling\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        try:\n",
        "            inputs = batch['input'].to(device)\n",
        "            crack_masks = batch['crack_mask'].to(device)\n",
        "            metadata = torch.stack([batch['notch_length'], batch['notch_position']], dim=1).to(device)\n",
        "\n",
        "            # Print shapes for first batch of first epoch for debugging\n",
        "            if epoch == 0 and batch_idx == 0:\n",
        "                print(f\"Input shape: {inputs.shape}\")\n",
        "                print(f\"Crack mask shape: {crack_masks.shape}\")\n",
        "                print(f\"Metadata shape: {metadata.shape}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, metadata)\n",
        "\n",
        "            if epoch == 0 and batch_idx == 0:\n",
        "                print(f\"Output shape: {outputs.shape}\")\n",
        "\n",
        "            loss = criterion(outputs, crack_masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Print batch progress every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            try:\n",
        "                inputs = batch['input'].to(device)\n",
        "                crack_masks = batch['crack_mask'].to(device)\n",
        "                metadata = torch.stack([batch['notch_length'], batch['notch_position']], dim=1).to(device)\n",
        "\n",
        "                outputs = model(inputs, metadata)\n",
        "                loss = criterion(outputs, crack_masks)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "            except Exception as e:\n",
        "                print(f\"Error in validation: {e}\")\n",
        "                continue\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print status\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save model if it's the best so far\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': best_val_loss,\n",
        "        }, 'best_crack_model.pth')\n",
        "        print(f\"Model saved at epoch {epoch+1} with val_loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping condition\n",
        "    if epoch > 10 and optimizer.param_groups[0]['lr'] < 1e-5:\n",
        "        print(\"Early stopping due to learning rate reduction\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe1aqRXEFgM0"
      },
      "outputs": [],
      "source": [
        "# Visualization test\n",
        "def test_dataset_visualization():\n",
        "    print(\"Testing dataset visualization:\")\n",
        "    sample_idx = 0\n",
        "    sample = dataset[sample_idx]\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    input_img = sample['input'].numpy().transpose(1, 2, 0)\n",
        "    plt.imshow(input_img)\n",
        "    plt.title(f\"Input (NL: {sample['notch_length'].item()*100:.1f}mm, NP: {sample['notch_position'].item()*100:.1f}mm)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    output_img = sample['output'].numpy().transpose(1, 2, 0)\n",
        "    plt.imshow(output_img)\n",
        "    plt.title(\"Ground Truth Output\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    crack_mask = sample['crack_mask'].numpy()[0]\n",
        "    plt.imshow(crack_mask, cmap='hot')\n",
        "    plt.title(\"Crack Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run visualization test\n",
        "test_dataset_visualization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaOrIVEXF0ym"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMMF/2D/3g38bMqeh9DWoui",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}